{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Woodridge\n",
    "\n",
    "Text book: Introductory Econometrics: A Modern Approach, 7th edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 The Nature of Econometrics and Economic Data\n",
    "\n",
    "#### Data\n",
    "\n",
    " - Experimental data\n",
    " - Nonexperimental data (aka, observational data or retrospective data)\n",
    "\n",
    "#### How do perform empirical economic analysis\n",
    " 1. **Construct a economic model**: consists of mathematical equations that describe various relationships\n",
    " 2. **Turn the economic model into a econometric model**\n",
    "    - The betas describe the directions and strengths of the relationshiop between the two variables.\n",
    "    - Dealing with the error term or disturbance term is perhaps the most important component of any econometrics analysis.\n",
    "\n",
    "#### Data Structure\n",
    " - **Cross-Sectional Data**: A sample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time.\n",
    "    - Often assume it is obtained by random sampling  \n",
    "    - Be careful when random sampling might be violated by, say, dependent draws  \n",
    " - **Time Series Data**: Observations on a variable or several variables over time.\n",
    "   - More difficult to analyze than cross-sectional data: \n",
    "     - Economic observations can rarely be assumed to be independent across time.\n",
    "     - Trending: highly persistent nature of many economic time series\n",
    " - **Pooled Cross Section**: Have both cross-sectional and time series features.\n",
    "   - Increases sample size\n",
    "   - See how a key relationship has changed over time\n",
    " - **Panel or Longitudinal Data**: A times series for each cross-sectional member in the dataset.\n",
    "   - Same cross-sectional units are followed over a given time period, can also be treated as a pooled cross section, where members happen to show up in each point in time\n",
    "   - Multiple observations on the same units allows us to control for certain unobserved characteristics of members\n",
    "   - Allow us to study the importance of lags in behavior or the result of decision making\n",
    "\n",
    "#### Other\n",
    " - **ceteris paribus**: other (relevant) factors being equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Regression Analysis with Cross-sectional Data\n",
    "\n",
    "## Chapter 2 The Simple Regression Model \n",
    "\n",
    "### 2.1 Definition of the simple regression model\n",
    "\n",
    "#### 1. The model\n",
    "\n",
    "We want to explain y in terms of x or study how y varies with changes in x.\n",
    "\n",
    "The simple linear regression model (aka two-variable linear regression model, or bivariate linear regression model) is defined by:\n",
    "\n",
    "> y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &mu;\n",
    "\n",
    "#### 2. Terms\n",
    "\n",
    " -  x and y\n",
    "\n",
    "|          Y         |           X          |\n",
    "|:------------------:|:--------------------:|\n",
    "| Dependent variable | Independent variable |\n",
    "| Explained variable | Explanatory variable |\n",
    "|  Response variable |   Control variable   |\n",
    "| Predicted variable |  predictor variable  |\n",
    "|     Regressand     |       Regressor      |\n",
    "\t\n",
    " - &beta;<sub>0</sub>: slope parameter  \n",
    " - &beta;<sub>1</sub>: intercept parameter, or constant term\n",
    " - &mu; the error term of disturbance: factors other than x that affect y. Or. the unsystematic part, i.e. the part of y not explained by x\n",
    " - &beta;<sub>0</sub> + &beta;<sub>1</sub>x: systematic part of y, i.e. part of y explained by x\n",
    "\n",
    "\n",
    "#### 3. Assumptions  \n",
    "\n",
    "1. WLOS, assume E(&mu;) = 0, as long as intercept is included in the equation. \n",
    "2. &mu; is **mean independent of** x: E(&mu;|x) = E(&mu;)\n",
    "\n",
    "=> the **zero conditional mean assumption**: E(&mu;|x) = 0\n",
    "\n",
    "Taking expectation conditional on x on both sides of y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &mu;\n",
    "\n",
    "=> **population regression function**: E(y|x) = &beta;<sub>0</sub> + &beta;<sub>1</sub>x\n",
    "\n",
    "### 2.2 Deriving the Ordinary Least Squares Estimates\n",
    "\n",
    "#### Method I\n",
    "\n",
    "Step 1: Based on two assumptions:\n",
    "\n",
    " - E(&mu;) = 0\n",
    " - E(&mu;|x) = E(&mu;)\n",
    "   - This implies zero covariance between &mu; and x. (see [proof](https://math.stackexchange.com/questions/967641/prove-that-mean-independent-random-variables-are-uncorrelated))\n",
    "   - so, Cov(&mu;, x) = E(&mu;x) - E(&mu;)E(x) = E(&mu;x) = 0\n",
    "\n",
    "Step 2: From E(&mu;) = 0 & E(&mu;x) = 0:\n",
    "\n",
    " - E(&mu;) = 0 => E(y - &beta;<sub>0</sub> - &beta;<sub>1</sub>x) = 0\n",
    " - E(&mu;x) = 0 => E[x(y - &beta;<sub>0</sub> - &beta;<sub>1</sub>x)] = 0\n",
    "\n",
    "Step 3: Replacing expectation with averages, and solve for &beta;<sub>0</sub>\n",
    " and &beta;<sub>1</sub>\n",
    " \n",
    "#### Method II\n",
    "\n",
    "To minimize the sum of squared residuals. Take the derivative of SSR w.r.t. &beta;<sub>0</sub> and &beta;<sub>1</sub>, get the **first order conditions** for OLS estimates\n",
    "\n",
    "#### Comments on the estimates: \n",
    "\n",
    " - The estimates are called the ordinary least squares estimates\n",
    " - The equation is called **sample regression function (SRF)**\n",
    " - &beta;<sub>1</sub> is just a scaled version of &rho;<sub>xy</sub>. In effect, simple regression is an analysis of correlation between two variables\n",
    " - If the estimates intercept is negative but y should always be position, it could be that the sample size near origin is too small, thus it's no surprising that the regression line does poorly at low values of x.\n",
    " - We run the regression of y on x, or regress y on x.\n",
    "\n",
    "### 2.3 Properties of OLS on Any Sample of Data\n",
    "\n",
    "From E(&mu;) = 0\n",
    "\n",
    " - The sum of the residuals is 0 => y̅ = ŷ-bar\n",
    " - The point (x̅, y̅) is always on the OLS regression line.\n",
    "\n",
    "From E(&mu;x) = 0\n",
    "\n",
    " - The sample covariance between x and residuals is 0.\n",
    " - Sample covariance between ŷ and residual is 0\n",
    "\n",
    "Goodness of fit\n",
    "\n",
    " - Total sum of squares (SST); explained sum of squares (SSE); residual sum of squares (SSR): SST = SSE + SSR. \n",
    " - R<sup>2</sup> = SSE/SST, assuming SST != 0\n",
    " - R<sup>2</sup> = &rho;<sub>y, ŷ</sub><sup>2</sup>\n",
    " - Seemingly low R-squared does not necessary mean that an OLS regression equation is useless. \n",
    "\n",
    "### 2.4 Units of Measurement and Functional Form\n",
    "\n",
    " - Changing units of x and y changes estimates in expected ways, and doesn't changed R<sup>2</sup>.\n",
    " - In the log-level model, 100&beta;<sub>1</sub> is called semi-elasticity of y w.r.t. x; In the log-log model, &beta;<sub>1</sub> is the elasticity of y w.r.t. x\n",
    " - Linear means the equation is linear in the parameters &beta;<sub>0</sub> and &beta;<sub>1</sub>.\n",
    "\n",
    "### 2.5 Expected Values and Variances of the OLS Estimators (statistical properties of OLS)\n",
    "\n",
    "##### SLR 1 Linear in parameters\n",
    "\n",
    " - y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &mu;\n",
    " - y, x, &mu; are random variables\n",
    " - &beta;<sub>0</sub> and &beta;<sub>1</sub> are not random variables. They're fixed values that we don't know.\n",
    "\n",
    "##### SLR 2 Random sampling\n",
    "\n",
    " - We have a random sample of size n.\n",
    " - To write y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &mu; in terms of the random sample as: y<sub>i</sub> = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>i</sub> + &mu;<sub>i</sub>\n",
    "\n",
    "##### SLR 3 Sample variation in the Explanatory Variable\n",
    " \n",
    " - The sample outcomes on x (i.e. x<sub>1</sub>, x<sub>1</sub>, ...) are not all the same value\n",
    "\n",
    "##### SLR 4 Zero Conditional Mean\n",
    "\n",
    " - E(&mu;|x) = 0\n",
    " - This is equivalent to E(&mu;|x) = E(&mu;) & E(&mu;) = 0\n",
    " - For a random sample, this implies E(&mu;<sub>i</sub>|x<sub>i</sub>) = 0 (&mu;<sub>i</sub> is r.v., x<sub>i</sub> is not)\n",
    "\n",
    "##### Theorem: using SLR 1—4, we can prove the OLS estimates of &beta;<sub>0</sub> and &beta;<sub>1</sub> are unbiased\n",
    "\n",
    "##### SLR 5 Homoskedasticity\n",
    "\n",
    " - Var(&mu;|x) = &sigma;<sup>2</sup>\n",
    " - We can derive: Var(&mu;) = &sigma;<sup>2</sup>, so &sigma;<sup>2</sup> is often called the error variance or disturbance variance.\n",
    "\n",
    "It's often useful to write SLR 4 & 5 as:\n",
    " \n",
    " - E(y|x) = &beta;<sub>0</sub> + &beta;<sub>1</sub>x\n",
    " - Var(y|x) = &sigma;<sup>2</sup>\n",
    "\n",
    "![](https://robertdkirkby.github.io/introtoeconometrics/Topic6_Heteroskedasticity/Material/Topic6_Fig1.png)\n",
    "\n",
    "##### Theorem: sample variances of OLS estimates\n",
    "\n",
    " - &beta;<sub>1</sub>-hat: &sigma;<sup>2</sup> / SST<sub>x</sub> \n",
    " - &beta;<sub>0</sub>-hat: &sigma;<sup>2</sup> * sum of x<sub>i</sub><sup>2</sup> / (n * SST<sub>x</sub>)\n",
    "\n",
    "##### Theorem: unbiased estimation of &sigma;<sup>2</sup>\n",
    "\n",
    " - E(SSR/(n-2)) = &sigma;<sup>2</sup>\n",
    " - sqrt(SSR/(n-2)) is called the standard error of the regression (SER), which is not an unbiased estimator of &sigma;, but an consistent estimator of &sigma;\n",
    "\n",
    "### 2.6 Regression through the Origin and Regression on a Constant\n",
    "\n",
    " - Regression through the origin is not done very often because if the intercept is not 0, then the estimator for &beta;<sub>1</sub> is biased.\n",
    " - What if we set the slope to zero and estimate an intercept only? Then the intercept is y̅. The constant that produces the smallest sum of squared deviations is always the sample average.\n",
    "\n",
    "### 2.7 Regression on a Binary Explanatory Variable\n",
    "\n",
    " - x is called a binary variable or dummy variable if x takes on only two values, zero and one.\n",
    "   - 𝛽̂<sub>0</sub> = y̅<sub>0</sub>\n",
    "   - 𝛽̂<sub>1</sub> = y̅<sub>1</sub> - y̅<sub>0</sub>\n",
    " - Small R<sup>2</sup> only means the variance in the unobservables, Var(&mu;), is large relative to Var(y)\n",
    " - Random sampling means that the data we obtain are independent, identically distributed draws from the population distribution represented by the random variables x and y.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Problem: There are two random variables: X and Y. We wan't to know how X affects Y. (this is not the same as causality).\n",
    "\n",
    "#### Step 1: Establish a model\n",
    "\n",
    " - Assume a linear (in parameters) relationship: Y = &beta;<sub>0</sub> + &beta;<sub>1</sub>X + U\n",
    " - We think that there's a lot of other stuff affecting Y, but we're only interested in X.\n",
    " - This relationship might be wrong. We have various criteria to see if it's a good modeling of the relationship.\n",
    "\n",
    "#### Step 2: collect data\n",
    "\n",
    " - Random sampling: this means the data are i.i.d. draws from the population\n",
    " - Why do we need random sampling?\n",
    "   - Intuitively, to make the sample representative of the whole population\n",
    "   - Algebraically, to prove unbiasedness we need independence of Us\n",
    " - How does the process work?\n",
    "   - Think of each individual as an experiment: (X<sub>i</sub>, Y<sub>i</sub>), each experiment are independent and all have the same distribution.\n",
    "   - Do the experiment n times, and we have the data (x<sub>i</sub>, y<sub>i</sub>)\n",
    "   - x<sub>i</sub> can't be all the same. If it is, all the data points are on a vertical line, there's no way you could get good estimates. If they turn out to be the same, you might want to do more experiments.\n",
    "\n",
    "#### Step 3: Run the model through data\n",
    "\n",
    " - Assume: E(U|X) = 0, which is the equivalent as the following:\n",
    "   - E(U) = 0. If it's not 0, then we can always redefine U and &beta;<sub>0</sub> to make it 0\n",
    "   - E(U|X) = E(U). This is stronger than uncorrelation and weaker than independence.\n",
    "     - We need the \"idea\" that U and X are uncorrelated. If part of X is in part of U, then we can't isolate X's effect.\n",
    "     - But uncorrelation only measures linear dependence. Even if U and X are uncorrelated, they can still be largely dependent.\n",
    "     - Mean independence suffices for our purposes. Independence is too strong an assumption.\n",
    " - E(U|X) = 0 also means E(Y|X) = &beta;<sub>0</sub> + &beta;<sub>1</sub>X, which, geometrically, means the Y values are distributed evenly on both sides of the modeling line.\n",
    " - Calculate the estimates:\n",
    "   - E(U) = 0 & E(U|X) = 0. Replacing the expection with averages.\n",
    "   - The results are in terms of (X<sub>i</sub>, Y<sub>i</sub>). You could also write it in terms of (x<sub>i</sub>, y<sub>i</sub>), which will already have some sense of conditioning.\n",
    "   - The results are the same as what we get from minimizing SSR, which is why the estimates are also called OLS estimates.\n",
    "\n",
    "#### Step 4: Assess the model (the estimates)\n",
    "\n",
    " - Unbiasedness: On average, do the estimates represent the real value of &beta;<sub>0</sub> and &beta;<sub>1</sub>?\n",
    "   - E(&beta;<sub>1</sub>-hat | X<sub>1</sub>=x<sub>1</sub> ...) = &beta;<sub>1</sub>\n",
    "   - E(&beta;<sub>0</sub>-hat | X<sub>1</sub>=x<sub>1</sub> ...) = &beta;<sub>0</sub>\n",
    "   - Conditioning on the values of Xs, but Ys are still random variables.\n",
    " - Variance: How far can those estimates be away from the real values on average?\n",
    "   - Assume homoskedasticity. Why?\n",
    "   - Get unbiased estimates of &sigma;<sup>2</sup>\n",
    "   - Get variance of estimates (conditioning on Xs)\n",
    "\n",
    "Question: Why don't we assume cov(x, &mu;) = 0 & E(&mu;) = 0 instead of SLR4? it seems that if we only assume these two conditions, we could still get the estimates and prove unbiasedness.\n",
    "\n",
    " - No. It doesn't lead to unbiasedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Multiple Regression Analysis: Estimation\n",
    "\n",
    "Why do we need multiple regression analysis?\n",
    "\n",
    " - The key assumption for SLR, that all other factors affecting y are uncorrelated with x, is often unrealistic.\n",
    " - If we add more factors to our model that are useful for explaning y, then more of the variation in y can be explained.\n",
    " - It can incorporate fairly general functional form relationship.\n",
    "\n",
    "### 3-1 Motivation for multiple regression\n",
    "\n",
    "Multiple linear regression (MLR) model (aka multiple regression model):\n",
    "\n",
    " - y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &beta;<sub>2</sub>x<sub>2</sub> + &beta;<sub>3</sub>x<sub>3</sub> + ... + &beta;<sub>k</sub>x<sub>k</sub> + &mu;\n",
    " - terms: \n",
    "   - &beta;<sub>0</sub>: intercept\n",
    "   - &beta;<sub>i</sub> are slope parameters\n",
    "   - u: error term or disturbance\n",
    " - key assumption: E(&mu;|x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub>) = 0 \n",
    "   - intuitively, this the key for estimating ceteris paribus relationship.\n",
    "   - algebraically, without this assumption, the estimators will be biased.\n",
    "   - from this we can derive that all xs are uncorrelated to &mu;\n",
    "\n",
    "### 3-2 Mechanics and interpretation of ordinary least squares\n",
    "\n",
    "The power of multiple regression analysis is that it provides partial effect or ceteris paribus interpretation even though the data have not been collected in a ceteris paribus fashion.\n",
    "\n",
    "To solve for estimates, must assume that the first order conditions can be solved uniquely for &beta;s. For example, to solve for &beta;<sub>1</sub>:\n",
    "\n",
    " - To derive &beta;<sub>1</sub>, regression x<sub>1</sub> on x<sub>2</sub>, x<sub>3</sub>, ... , x<sub>k</sub>, get the residuals. The residuals are part of  x<sub>1</sub> that's uncorrelated with other xs. Another way of saying this is that residuals are x<sub>1</sub> after the effects of other xs have been partialled out or netted out.\n",
    " - Then regress y on the residuals\n",
    " - In econometrics, the general partialling out result is usually called the Frisch-Waught theorem.\n",
    "   \n",
    "The simple regression of y on x<sub>1</sub> and the multiple regression of y on x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub> produce an identical estimate of x<sub>1</sub> only if:\n",
    "\n",
    " - the OLS coefficients on x<sub>2</sub> through x<sub>k</sub> are all zero\n",
    " - or x<sub>1</sub> is uncorrelated with each of x<sub>2</sub>, ... , x<sub>k</sub>\n",
    "\n",
    "An important fact about R<sup>2</sup> is that it never decreases, and it usually increases, when another independent variable is added to a regression and the same set of observations is used for both regressions:\n",
    "\n",
    " - the variance of y doesn't change\n",
    " - SSR only decreases. (the model with less vars is just a special case of the one with more vars)\n",
    "\n",
    "Generally, a low R<sup>2</sup> indicates that it is hard to predict individual outcomes on y with much accuracy.\n",
    "   \n",
    "Regression through the origin:\n",
    "\n",
    " - OLS residuals no longer have a zero sample average\n",
    " - R<sup>2</sup> can be negative. This means the sample average explains more of the variation in the y than the explanatory variables. Either we should include an intercept in the regression or conclude that the explanatory variables poorly explain y.\n",
    " - If the intercept in the population model is different from 0, then OLS estimators of the slope parameters will be biased.\n",
    " - The upside to regression through the origin is that the variances of the estimators are smaller.\n",
    "\n",
    "### 3-3 The expected value of the OLS estimators\n",
    "\n",
    "Statistical properties have nothing to do with a particular sample, but rather with the property of estimators when random sampling is done repeatedly.\n",
    "\n",
    "**Assumption MLR.1 Linear in parameters**\n",
    "\n",
    " - The model in the population can be written as: y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &beta;<sub>2</sub>x<sub>2</sub> + &beta;<sub>3</sub>x<sub>3</sub> + ... + &beta;<sub>k</sub>x<sub>k</sub> + &mu;\n",
    "   - &beta;<sub>0</sub>, &beta;<sub>1</sub>, ... , &beta;<sub>k</sub> are the unknown parameters (constants) of interest\n",
    "   - &mu; is an unobserved random error or disturbance term.\n",
    " - This is called the **population model** or **true model**\n",
    "\n",
    "**Assumption MLR.2 Random sampling**\n",
    "\n",
    " - We have a random sample of n observations {(x<sub>i1</sub>, x<sub>i2</sub>, ... , x<sub>ik</sub>, y<sub>i</sub>): i=1, 2, ... , n}\n",
    "\n",
    "**Assumption MLR.3 No perfect collinearity**\n",
    "\n",
    " - In the sample (and therefore in the population), there are no exact linear relationships among the independent variables.\n",
    "   - If an independent variable is an exact linear combination of the other independent variables, then we say the model suffers from **perfect collinearity**, and it cannot be estimated by OLS.\n",
    "     - Intuitively, the problem is that we cannot interpret the equation in a ceteris paribus fashion.\n",
    "     - Algebraically, suppose x<sub>1</sub> is determined by other variables, then regressing x<sub>1</sub> on other variables will give us residuals all 0. Then when we try to compute &beta;<sub>1</sub>, the denominator will be 0.\n",
    "     - Possible solution: drop some variables from the model\n",
    "   - This assumption does allow the independent variables to be correlated; they just cannot be perfectly correlated.\n",
    " - Violations to this assumption\n",
    "   - One of the independent variable is constant\n",
    "   - The sample size is too small in relation to the number of parameters being estimated. (n < k+1)\n",
    "\n",
    "**Assumption MLR.4 Zero conditional mean**\n",
    "\n",
    " - The error &mu; has an expected value of zero given any values of the independent variables: E(&mu;|x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub>) = 0.\n",
    "   - When MLR.4 holds, we often say that we have **exogenous explanatory variables**.\n",
    "   - If x<sub>j</sub> is correlated with &mu;for any reason, then x<sub>j</sub> is said to be an **endogenous explanatory variable**.\n",
    " - Violations to this assumption\n",
    "   - The functional relationship between the explained and explanatory variables is misspecified.\n",
    "   - Omitting an important factor that is correlated with any of x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub> causes MLR.4 to fail.\n",
    "\n",
    "**Theorem 3.1 Unbiasedness of OLS**\n",
    "\n",
    " - Under assumption MLR.1-4, all estimates are unbiased.\n",
    " - We do not mean any particular estimate is unbiased. We mean that the procedure by which the OLS estimates are obtained is unbiased when we view the procedure as being applied across all possible random samples.\n",
    "\n",
    "**Some issues with MLR**\n",
    "\n",
    " - **Inclusion of an irrelevant variable** or **overspecifying the model**: This does not affect the unbiasedness of the OLS estimators.\n",
    " - **Excluding a relevant variable** or **underspecifying the model**\n",
    "   - Expectation — true value is called **omitted variable bias**\n",
    "   - Correlation between a single explanatory variable and the error generally results in all OLS estimators being biased.\n",
    "   - We can not say a particular estimate is too big or too small. We can only say if we collect many random samples and obtain the simple regression estimates each time, then the average of these estimates will be greater than or less than their true values.\n",
    "   - direction of bias\n",
    "     - expectation > true value: **upward bias**\n",
    "     - expectation < true value: **downward bias**\n",
    "     - 0 < abs(expectation) < abs(true value): **biased toward zero**\n",
    "\n",
    "### 3-4 The variance of the OLS estimators\n",
    "\n",
    "**Assumption MLR.5 Homoskedasticity**\n",
    "\n",
    " - The error &mu; has the same variance given any value of the explanatory variables: Var(&mu;|x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub>) = &sigma;<sup>2</sup>\n",
    " - We need this for two reasons:\n",
    "   - The formulas of variance of &beta;s are simplified by imposing the constant error variance assumption.\n",
    "   - OLS has an important efficiency property if we add the homoskedasticity assumption.\n",
    " - MLR.1—MLR.5 are collectively known as the Gauss-Markov assumptions (for cross-sectional regression)\n",
    "\n",
    "**Theorem 3.2 Sampling variances of the OLS slope estimators**\n",
    "\n",
    " - The variance of estimate of &beta;<sub>j</sub> = &sigma;<sup>2</sup> / SST<sub>j</sub>(1 - R<sub>j</sub><sup>2</sup>)\n",
    "   - SST<sub>j</sub> is the total sample variance in x<sub>j</sub>\n",
    "   - R<sub>j</sub><sup>2</sup> is the R-squared from regressing x<sub>j</sub> on all other independent variables (including an intercept)\n",
    "   - square root of this is called **standard deviation** of &beta;<sub>j</sub>-hat\n",
    "   - replacing &sigma; with its estimate gives us the **standard error** of &beta;<sub>j</sub>-hat\n",
    " - Is there a simple formula for the variance where we do not condition on the sample outcomes of the explanatory variables?\n",
    "   - None that is useful. The above formula is a highly nonlinear function of xs, making averaging out across the population distribution of the explanatory variables virtually impossible.\n",
    " - Remember, all of the Gauss-Markov assumptions are used in obtaining this formula.\n",
    " - Components of the formula:\n",
    "   - &sigma;<sup>2</sup>: the error variance. For a given dependent y, there is really only one way to reduce the error variance, and that is to add more explanatory variables to the equation (take some factors out of the error term)\n",
    "   - SST<sub>j</sub>: the total sample variation in x<sub>j</sub>. We can increase the sample size to increase SST<sub>j</sub> and reduce the variance.\n",
    "   - R<sub>j</sub><sup>2</sup>: the linear relationships among the independent variables\n",
    "     - R<sub>j</sub><sup>2</sup> = 0 iff x<sub>j</sub> has zero sample correlation with every other independent variable\n",
    "     - High (but not perfect) correlation between two or more independent variables is called **multicollinearity**.\n",
    " - The choice of whether to include a particular variable in a regression model can be made by analyzing the tradeoff between bias and variance. But we would prefer including the variable because:\n",
    "   - (main reason) The bias doesn't shrink to 0 as the sample size grows (it doesn't follow any pattern), but the variance shrinks to 0 as sample size grows.\n",
    "   - &sigma;<sup>2</sup> increases when a variable is excluded from the model. Simply comparing between &sigma;<sup>2</sup> / SST<sub>j</sub>(1 - R<sub>j</sub><sup>2</sup>) and &sigma;<sup>2</sup> / SST<sub>j</sub> ignores this fact.\n",
    "\n",
    "**Theorem 3.3 Unbiased estimation of &sigma;<sup>2</sup>**\n",
    "\n",
    " - Under Gauss-Markov assumptions MLR.1—MLR.5, E(SSR/(n-k-1)) = &sigma;<sup>2</sup>\n",
    "   - n-k-1 is the degrees of freedom: #obervations-#number of estimated parameters.\n",
    "   - sqrt(SSR/(n-k-1)) is called the standard error of the regression (SER), or standard error of the estimate, or the root mean squared error.\n",
    "\n",
    "### 3-5 Efficiency of OLS: The Guass-Markov Theorem\n",
    "\n",
    "**Theorem 3.4 Guass-Markov Theorem**\n",
    "\n",
    " - Under Assumptions MLR.1through MLR.4, the estimators for &beta;s are the best linear unbiased estimators\n",
    "   - best: having the smallest variance\n",
    "   - linear: it can be expressed as a linear function of the data on the dependent variable\n",
    "   - estimator: it is a rule that can be applied to any sample of data to produce an estimate\n",
    " - If we want to estimate any linear function of the &beta;<sub>j</sub>, then the corresponding linear combination of the OLS estimators achieves the smallest variance among all linear unbiased estimators.\n",
    "\n",
    "### 3-6 Some comments on the language of multiple regression analysis\n",
    "\n",
    "It's wrong to say we \"estimated an OLS model\"\n",
    "\n",
    " - OLS is an estimation method, not a model\n",
    " - A model describes an underlying population and depends on unknown parameters\n",
    " - We should say, we \"estimated a linear model by OLS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Multiple Regression Analysis: Inference\n",
    "\n",
    "### 4-1 Sampling distributions of the OLS estimators\n",
    "\n",
    "**Assumption MLR.6 Normality assumption**\n",
    "\n",
    " - The population error &mu; is independent of the explanatory variables, and &mu; ~ Normal (0, &sigma;<sup>2</sup>)\n",
    " - MLR.6 implies MLR.4 and MLR.5\n",
    "   - E(&mu;|x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub>) = E(&mu;) = 0\n",
    "   - Var(&mu;|x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>k</sub>) = Var(&mu;) = &sigma;<sup>2</sup>\n",
    " - Assumptions MLR.1—MLR.6 are called **classical linear model (CLM)** assumptions\n",
    " - The model under these six assumptions is called the **classical linear model**\n",
    " - Under CLM assumptions, the OLS estimators have a stronger efficiency property than they would under Gauss-Markov assumptions\n",
    "   - OLS estimators has the smallest variance among unbiased estimators\n",
    "\n",
    "**Theorem 4.1 Normal sampling distributions**\n",
    "\n",
    " - Under CLM assumptions MLR.1—MLR.6, conditional on the sample values of the independent variables:\n",
    "   - $ \\hat{\\beta_j} \\sim Normal(\\beta_j, Var(\\hat{\\beta_j}))  $\n",
    "   - $ \\frac{\\hat{\\beta_j} - \\beta_j}{sd(\\hat{\\beta_j})} \\sim Normal(0, 1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2 Testing hypotheses about a single population parameter: the t test\n",
    "\n",
    "**Theorem 4.2 t distributions for the standardized estimators**\n",
    "\n",
    " - Under CLM assumptions MLR.1—MLR.6:\n",
    "   - $ \\frac{\\hat{\\beta_j} - \\beta_j}{se(\\hat{\\beta_j})} \\sim t_{n-k-1} $\n",
    "   - The t statistic or the t ratio of $\\hat{\\beta_j}$ is defined as: $ t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{se(\\hat{\\beta_j})} $\n",
    "   - It is helpful to index t statistics using the name of the independent variable, such as $ t_{educ} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2a Testing against one-sided alternatives\n",
    "\n",
    " - Hypotheses\n",
    "   - **Alternative hypothesis** (**one sided alternative**): $ H_1\\colon \\beta_j > 0 $\n",
    "   - **Null hypothesis**: $ H_0\\colon \\beta_j = 0 $\n",
    "     - This means once $ x_1, x_2, ..., x_{j-1}, x_{j+1}, ..., x_k $ have been accounted for, $x_j$ has no effect on the expected value of y.\n",
    "   - We are testing hypotheses about the population parameters, we are not testing hypotheses about the estimates from a particular sample.\n",
    "\n",
    " - Rejection rule\n",
    "   - significance level (or level) p: the probability of rejecting $ H_0 $ when it is in fact true\n",
    "   - critical value (c): \n",
    "     - in stata, use `invttail(df, p)`\n",
    "     - for degrees of freedom greater than 120, one can use the standard normal critical values\n",
    "   - if $ t_{\\hat{\\beta_j}} > c $, $ H_0 $ is rejected in favor of $ H_1 $ at the chosen significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2b Two-sided alternatives\n",
    "\n",
    " - Hypotheses\n",
    "   - Null hypothesis: $ H_0\\colon \\beta_j = 0 $\n",
    "   - Two-sided alternative: $ H_1\\colon \\beta_j \\neq 0 $\n",
    " - Rejection rule\n",
    "   - If $ |t_{\\hat{\\beta_j}}| > c $, $ H_0 $ is rejected in favor of $ H_1 $ at the ...% level, we say that $x_j$ is statistically significant, or $x_j$ is statistically different from zero, at the ...% level.\n",
    "   - If $ H_0 $ is not rejected, we say that $x_j$ is statistically insignificant at the ...% level\n",
    "   - critical value c: `invttail(df, p/2)`\n",
    "   - If a t statistic is significant at very small significance levels, we say a variable is statistically significant at any conventional significance level.\n",
    "   - If |coefficient| is small, we say it's practically insignificant or economically insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2c Testing other hypothees about $\\beta_j$\n",
    "\n",
    " - Hypotheses\n",
    "   - Null hypothesis: $ H_0\\colon \\beta_j = a_j $\n",
    "   - Two-sided alternative: $ H_1\\colon \\beta_j \\neq a_j $\n",
    " - t statistic: $ \\frac{\\hat{\\beta_j} - a_j}{se(\\hat{\\beta_j})} \\sim t_{n-k-1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2d Computing p-values for t tests\n",
    "\n",
    " - p-value\n",
    "   - smallest significance level at which the null hypothesis would be rejected\n",
    "   - P $(|T| > |t|)$\n",
    "   - the one-sided p-value is obtained by dividing the two-sided p-value by 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2e A reminder on the language of classical hypothesis tesing\n",
    "\n",
    " - When $H_0$ is not rejected\n",
    "   - We fail to reject $H_0$ at the x% level\n",
    "   - NOT: $H_0$ is accepted at the x% level. (There are other values that also can't be rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2f Economic, or practical, versus statistical significance\n",
    "\n",
    " - Small sample size often results in statistical insignificance\n",
    "   - standard errors of $\\hat{\\beta}$s are larger (less precise estimators)\n",
    "   - critical values are larger in magnitude\n",
    " - Guidelines for discussing economic and statistical significance of a variable\n",
    "   1. If the variable is statistically significant:\n",
    "     - If the sign is expected: discuss the magnitude of the coefficient to get an idea of its practical or economic importance.\n",
    "     - If the sign is not expected and its practical effect is large: often due to omitted variable bias or sth else.\n",
    "   2. If the variable is statistically insignificant:\n",
    "     - If the sign is expected: look at the magnitude. If it's large, then compute a p-value for the t statistic. (For small sample sizes, you can make a case for p-values as large as 0.20)\n",
    "     - If the estimates have the wrong sign: we ignore this and conclude the variables are statistically insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 Confidence intervals\n",
    "\n",
    " - confidence interval (CI) or interval estimates for the population estimates\n",
    "   - $ \\hat{\\beta_j} \\pm c \\cdot se(\\hat{\\beta_j}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4 Testing hypotheses about a single linear combination of the parameters\n",
    "\n",
    " - raarrange the equation by defining the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5 Testing multiple linear restrictions: the T test\n",
    "\n",
    "#### Testing exclusion restrictions\n",
    "\n",
    " - Model:\n",
    "   - Unrestricted model: $ y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k + \\mu $\n",
    "   - Restricted model: $ y = \\beta_0 + \\beta_1x_1 + ... + \\beta_{k-q}x_{k-q} + \\mu $\n",
    " - Hypothesis:\n",
    "   - Null hypothesis: $H_0 \\colon \\beta_{k-q+1} = 0, ..., \\beta_k = 0$ (k exclusion restrictions)\n",
    "   - Alternative hypothesis: $H_1 \\colon H_0$ is not true\n",
    " - Test: multiple hypotheses test or joint hypotheses test\n",
    "   - F statistic (F ratio): $\\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} = \\frac{(R_{ur}^2 - R_r^2)/q}{(1 - R_{ur}^2/(n-k-1)} \\sim F_{q,n-k-1}$  (think of q as $df_r - df_{ur}$)\n",
    "     - Be sure to use the same set of data when there're missing values!\n",
    "   - We reject $H_0$ in favor of $H_1$ at the chosen significance level if $F > c$, and say $x_{k-q+1}, ..., x_k$ are jointly statistically significant (or just jointly significant)\n",
    "   - p-value = P($\\mathscr{F} > F$)\n",
    " - Some potential questions:\n",
    "   - jointly significant but each variable is insignificant?\n",
    "     - might be the variables are highly correlated, and multicollinearity makes it difficult to uncover the partial effects of each variable\n",
    "   - What if q = 1? Does that mean we have two ways to test significance of a single variable?\n",
    "     - No. $t_{n-k-1}^2$ has an $ F_{1,n-k-1} $ distribution\n",
    "   - jointly insignificant variables + a significant variable $\\rightarrow$ jointly insignificant?\n",
    "     - This could happen. This gives an example of why we should not accept null hypotheses, we just fail to reject them.\n",
    "\n",
    "#### Special case (testing overall significance of the regression)\n",
    "\n",
    " - Model:\n",
    "   - Unrestricted model: $ y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k + \\mu $\n",
    "   - Restricted model: $ y = \\beta_0 + \\mu $\n",
    " - Hypothesis:\n",
    "   - Null hypothesis:\n",
    "     - $H_0 \\colon \\beta_1 = \\beta_2 = ... = \\beta_k = 0$ (k exclusion restrictions)\n",
    "     - or: $H_0 \\colon x_1, x_2, ..., x_k $ do not help to explain y\n",
    "     - or: $ E(y|x_1, x_2, ..., x_k) = E(y) $\n",
    "   - Alternative hypothesis: $H_1 \\colon H_0$ is not true\n",
    " - F statistic: $\\frac{R^2/k}{(1-R^2)/(n-k-1)}$ ($R_r^2 = 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6 Reporting regression results\n",
    "\n",
    " - For key variables, you should interpret the estimated coefficients (economic significance)\n",
    " - The standard errors should be included along with the estimated coefficients\n",
    " - The R-squared from the regression should always be included. \n",
    " - Reporting the SSR and the SER is sometimes a good idea, but not crucial.\n",
    " - The number of obervations used in estimating any equation should appear near the estimated equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Multiple Regression Analysis: OLS Asymptotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Consistency\n",
    "\n",
    " - the minimal requirement for an estimator\n",
    " - Under assumption MLR.1-4, $\\hat{\\beta_j}$ is unbiased. If this estimator is consistent, then the distribution of $\\hat{\\beta_j}$ becomes more and more tightly distributed around $\\beta_j$ as the sample size grows. As n tends to infinity, the distribution collapses to the single point $\\beta_j$\n",
    "\n",
    "#### Theorem 5.1 Consistency of OLS\n",
    " - Under Assumptions MLR.1-MLR.4, the OLS estimator $\\hat{\\beta_j}$ is consistent for $\\beta_j$, for all j = 0, 1, ..., k.\n",
    "   - In deriving consistency, two assumptions can be relaxed:\n",
    "     - MLR.3: We only need to assume there's no perfect collinearity in the population.\n",
    "     - MLR.4' Zero mean and zero correlation: $E(\\mu) = 0$ and $Cov(x_i, \\mu) = 0$ for j = 0, 1, ..., k\n",
    "   - correlation between $\\mu$ and any of independent variables generally causes all of the OLS estimators to be inconsistent.\n",
    "   - Why have we been using MLR.4 instead of MLR.4'?\n",
    "     - OLS turns out to be biased (but consistent) under Assumption MLR.4' if $E(\\mu|x_1, x_2, ..., x_k)$ depends on any of the $x_j$\n",
    "     - MLR.4 implies that we have properly modeled the population regression function (PRF): $E(y|x_1, ..., x_k) = \\beta_o + \\beta_1x_1 + ... + \\beta_kx_k$\n",
    "      \n",
    "#### Inconsistency\n",
    "\n",
    " - Practically, inconsistency can be viewed as the same as the bias:\n",
    "   - consistency is expressed in terms of the population variance of $x_1$ and the population covariance between $x_1$ and $x_2$\n",
    "   - bias is based on their sample counterparts\n",
    " - Inconsistency doesn't go away by adding more observations to the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Asymptotic normality and large sample inference\n",
    "\n",
    "#### Theorem 5.2 Asymptotic normality of OLS\n",
    "\n",
    " - Under Gauss-Markov Assumptions MLR.1-MLR.5\n",
    "   - $\\hat{\\beta_j}$ is asymptotically normally distributed: $\\sqrt{n}(\\hat{\\beta_j} - \\beta_j) \\overset{a}{\\sim} Normal(0, \\frac{\\sigma^2}{a_j^2})$, where $a_j^2 = plim(\\frac{\\sum_{i=1}^{n} \\hat{r_{ij}}^2 }{n})$, and $ \\hat{r_{ij}} $ are the residuals from regressing $x_j$ on the other independent variables\n",
    "     - This implies the F statistics have approximate F distributions in large sample sizes\n",
    "   - $\\hat{\\sigma}^2$ is a consistent estimator of $\\sigma^2$\n",
    "     - This implies $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$\n",
    "   - For each j,\n",
    "     - $\\frac{(\\hat{\\beta_j} - \\beta_j)}{sd(\\hat{\\beta_j})} \\overset{a}{\\sim} Normal(0, 1)$\n",
    "     - $\\frac{(\\hat{\\beta_j} - \\beta_j)}{se(\\hat{\\beta_j})} \\overset{a}{\\sim} Normal(0, 1)$\n",
    "\n",
    "#### The Lagrange multiplier (LM) statistic (or n-R-squared statistic or score statistic)\n",
    "\n",
    " - Steps\n",
    "   - Regress y on the restricted set of independent variables and save the residuals, $\\widetilde{\\mu}$\n",
    "   - Regress $\\widetilde{\\mu}$ on all of the independent variables and obtain the R-squared: $R_u^2$\n",
    "   - Compute $LM = nR_u^2$, and compare it to the appropriate critical value, c, in a $\\chi_q^2$ distribution.\n",
    " - Asymptotically, the LM statistic and the F statistic have the same probability of Type I error (reject null when null is true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Asymptotic efficiency of OLS\n",
    "\n",
    "A range of consistent estimators is obtained by solving:\n",
    "\n",
    " - $ \\sum_{i=1}^n g_j(\\boldsymbol{x_i})(y_i - \\widetilde{\\beta_0} - \\widetilde{\\beta_1}x_{i1} - ... - \\widetilde{\\beta_k}x_{ik}) = 0, j = 0, 1, ..., k $\n",
    " - g_j(\\boldsymbol{x_i}) denotes any function of all explanatory variables for observation i\n",
    "   - We obtain OLS estimators when $ g_0(\\boldsymbol{x_i}) = 1 $ and $ g_j(\\boldsymbol{x_i}) = x_{ij} $ for j = 0, 1, ..., k\n",
    "   \n",
    "#### Theorem 5.3 Asymptotic efficiency of OLS\n",
    " - Under the Gauss-Markov assumptions, the above range of estimators and OLS estimators are both asymptotically normally distributed and OLS estimators have the smallest asymptotic variances:\n",
    "   - $Avar\\sqrt{n}(\\hat{\\beta_j} - \\beta_j) \\leq Avar\\sqrt{n}(\\widetilde{\\beta_j} - \\beta_j)$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6 Multiple Regression Analysis: Further Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1 Effects of Data Scaling on OLS Statistics\n",
    "\n",
    " - Changing the unit of measurement only changes things that are important.\n",
    " - If one variable is in log form (whether it's y or $x_k$), changing the units doesn't affect the coefficient, but only changes the intercept\n",
    "   - Algebraically speaking, this is because $log(cx) = log(c) + log(x)$\n",
    "   - Intuitively speaking, the interpretation on a variable of log form is about its percentage change. This is the same no matter how you measure the variable.\n",
    " - Beta coefficients\n",
    "   - z-score: $\\frac{x - avg(x)}{sd(x)}$\n",
    "   - standardized coefficients or beta coefficients: estimates from regressing with all variables standardized (it is not necessary to include an intercept, it will be 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2 More on functional form\n",
    "\n",
    " - $log(y) = \\beta_0 + \\beta_1x + u$\n",
    "   - When x increases by n, the change in y should be $ e^(\\hat{\\beta_1}n) - 1 $, exactly\n",
    "   - $ e^(\\hat{\\beta_1}) - 1 $ is a consistent estimator of $ e^{\\beta_1} - 1 $\n",
    " - Reasons for using logs:\n",
    "   - Appealing interpretations\n",
    "   - Can be ignorant of the units of measurement\n",
    "   - log(y) only satisfy the CLM assumptions more closely than the level of y\n",
    "   - Taking the log of a variable often narrows its range so it's less sensitive to outliers\n",
    " - Rules of thumb for whether to take logs:\n",
    "   - When a variable is a positive dollar amount, the log is often taken\n",
    "   - Variables such as population, total number of employees, and school enrollment often appear in log form\n",
    "   - Variables that are measured in years usually appear in their original form\n",
    "   - A variable that is a proportion or a percent can appear in either original or log form\n",
    " - Percentage change v.s. percentage point change: umemployment goes from 8% yo 9%\n",
    "   - percentage change: 12.5%\n",
    "   - percentage point change: 1%\n",
    " - Limitation on log\n",
    "   - log cannot be used if a variable takes on zero or negative values\n",
    "   - In cases where $ y \\geq 0 $, log(1+y) is sometimes used\n",
    "     - when the data on y contain relatively few zeros, using log(1+y) can interpreting the estimates as if the variable were log(y) is acceptable\n",
    " - Interaction effect: $ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2$\n",
    "   - APE (average partial effect) (of $x_1$ on y): $\\hat{\\beta_1} + \\hat{\\beta_3}\\bar{x_2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
